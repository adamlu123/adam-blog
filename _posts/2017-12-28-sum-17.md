---
title: Deep Learning in 2017
Author: Adam
background: '/img/bg-post0.jpg'
---

2017 is another productive year in machine learning (especially deep learning) world. Several blogs have been written to summarize the new research ideas and development in the past year. I recommend the blogs by [Alex](https://blog.goodaudience.com/ai-in-2018-for-researchers-8955df0caaf9) and [Eduard](https://blog.statsbot.co/deep-learning-achievements-4c563e034257). They both mention some advancements in computer vision, NLP and GANs. 

Having been in the stats department at UC Irvine for a little more than one year, I feel like I am on my way of becoming a (religiously) bayesian. When a new model comes in, the first question pops up in mind must be: can it be bayesian?

So, as you might have guessed, this post will focus on the development of bayesian ideas in deep learning. Generally, there are a couple of directions that incorporate bayesian ideas: bayesian optimatizaiton (hyperparameter tuning), weight uncertainty in NN, output uncertainty quantification.  I would like to point out that generally, there are two main playgrounds of deep learning: model and alogorithms. Bayesian methods belongs to the second category, that is, whenever you have a model (MLP, CNN, RNN, GAN), you can always do inference in a bayesian fashion. (to be contd)


